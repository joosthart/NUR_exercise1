\section{Redshift distribution of galaxies}
In this exercise the redshift distribution of galaxies is calculated using cross-correlating galaxy positions with spectroscopic sources, with known redshifts. Galaxies are most likely to be at the redshift of sources they cluster strongest with. The cross-correlation signal $w$ of a galaxy sample $g$ with spectroscopic sample $s$ in a redshift bin $z_i$, can be written as a linear combination of the cross-correlations of the spectroscopic sources with those at different redshifts:

\begin{equation}
    \label{eq:cc}
    w_{gs}(z_i) = \sum_j f_j w_{ss}(z_i, z_j).
\end{equation}

$f_j$ is here the fraction of galaxies that are in redshift bin $z_j$. 

In this exercise, an idealized data, containing $w_{gs}$ and $w_{ss}$ set is used, which has a unique solution for $f_j$. The dataset has 16 redshift bins equally spaced between 0 and 0.8. 

\subsection*{2a}
The goal is to find a solution for the unknown $f_j$. Note that equation \ref{eq:cc} can bew written in matrix form as follows:
\begin{equation}
    W_{gs} = W_{ss}\mathbf{f}.
\end{equation}
This equation can be solved for $\mathbf{f}$ numerically using an LU decomposition. To calculate the LU matrix, Crout's algorithm is used. The L and U matrices are stored in one $16 \times 16$ array, using the fact that the diagonal of the L matrix only contains 1s.

Inspection of the matrix $W_{ss}$ showed that the maximum row values are already on the diagonal. Therefore, no pivoting is needed. Ones obtained the LU matrix, the system is solved for $\mathbf{f}$ using first forward substitution on the upper half of the LU matrix, followed by backward substitution of the lower half. The LU matrix and the solution for $\mathbf{f}$ are written to the terminal when running \texttt{run.sh}.

\subsection*{2b}
In this exercise, a single iterative improvement on the estimation of $\mathbf{f}$ is performed. The estimation, $\hat{\mathbf{f}}$, is the sum of the true solution and an error vector. An error of the estimation can be calculated with
\begin{equation}
    \mathbf{f}_\delta = W_{ss}\mathbf{\hat{f}} - W_{gs},
\end{equation}
where $\mathbf{\hat{f}}$ is the estimated $\mathbf{f}$ and $\mathbf{f}_\delta$ the difference with the real solution. In the perfect case $\mathbf{f}_\delta=0$. Since there are always errors involved with numeric analysis, generally $\mathbf{f}_\delta\neq 0$. $\hat{\mathbf{f}}$ can thus be descibed as
\begin{equation}
    \hat{\mathbf{f}} = \mathbf{f} + \mathbf{\delta}_f.
\end{equation}
Using the above we can write
\begin{equation}
    W_{ss}\hat{\mathbf{f}} = W_{ss}(\mathbf{f} + \mathbf{\delta}_f) = W_{gs}.
\end{equation}
This can be rewritten to obtain
\begin{equation}
    W_{ss}\mathbf{\delta}_f = W_{ss}\mathbf{\hat{f}} - W_{gs} = \hat{\mathbf{f}}_\delta.
\end{equation}
We can thus solve for $\mathbf{\delta}_f$ and subtract this from our initial estimate $\hat{\mathbf{f}}_\delta$.

This method is used in the code below to calculate a first order improvement on the estimate. My first version of the code used for-loops to solve the equations using the generated LU-decomposition. I later realized that I could also do it with matrix operations and implemented that, since it is more efficient. In my first version, the estimate improved a few orders of magnitude when doing a first order improvement. However, in my new version, the fist estimate was significantly better than with the for-loop method. A single iterative improvement makes the prediction worse in my latest version of the code. I think that this is due to the accumulation of errors in the for-loop method. The current method with matrix operations might accumulate less round off errors while solving the system. The sum of $\hat{\mathbf{f}}$ of the first estimate is just $\sim 10^-8$ different from 1. This is close to single point precision. A single iterative improvement would not improve the estimate, since it is machine error limited.

The code is also tested with double point precision. In this case single iterative improvement does improve the estimate.



\subsection*{Code}

\lstinputlisting{problem2.py}


\newpage
